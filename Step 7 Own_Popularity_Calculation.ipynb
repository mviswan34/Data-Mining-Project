{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"Popular_Clean.csv\")\n",
    "df2 = pd.read_csv(\"Song_Analytics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>...</th>\n",
       "      <th>highest_peak_value</th>\n",
       "      <th>normalized_likes</th>\n",
       "      <th>normalized_views</th>\n",
       "      <th>normalized_us_hit</th>\n",
       "      <th>normalized_high_peak</th>\n",
       "      <th>likes_per_day</th>\n",
       "      <th>views_per_day</th>\n",
       "      <th>genre_averaged_likes</th>\n",
       "      <th>normalized_num_peak_periods</th>\n",
       "      <th>popularity_score_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blinding Lights</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.00146</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.334</td>\n",
       "      <td>171.005</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.078045</td>\n",
       "      <td>20.463609</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>4.615121</td>\n",
       "      <td>0.011874</td>\n",
       "      <td>0.015113</td>\n",
       "      <td>0.025767</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>43.660471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.69000</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.514</td>\n",
       "      <td>98.029</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.554820</td>\n",
       "      <td>21.452550</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>0.029205</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>41.465744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        track_name  artist_name  danceability  energy  speechiness  \\\n",
       "0  Blinding Lights   The Weeknd         0.514   0.730       0.0598   \n",
       "1     Dance Monkey  Tones And I         0.824   0.587       0.0937   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  valence    tempo  ...  \\\n",
       "0       0.00146          0.000095    0.0897    0.334  171.005  ...   \n",
       "1       0.69000          0.000105    0.1490    0.514   98.029  ...   \n",
       "\n",
       "   highest_peak_value normalized_likes  normalized_views  normalized_us_hit  \\\n",
       "0               100.0        16.078045         20.463609           3.258097   \n",
       "1                12.0        16.554820         21.452550           2.484907   \n",
       "\n",
       "   normalized_high_peak likes_per_day views_per_day  genre_averaged_likes  \\\n",
       "0              4.615121      0.011874      0.015113              0.025767   \n",
       "1              2.564949      0.010971      0.014216              0.029205   \n",
       "\n",
       "  normalized_num_peak_periods  popularity_score_scaled  \n",
       "0                    0.333333                43.660471  \n",
       "1                    0.111111                41.465744  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>tempo</th>\n",
       "      <th>mean_zcr</th>\n",
       "      <th>median_zcr</th>\n",
       "      <th>std_zcr</th>\n",
       "      <th>max_zcr</th>\n",
       "      <th>aboveThr_zcr</th>\n",
       "      <th>mean_sc</th>\n",
       "      <th>median_sc</th>\n",
       "      <th>std_sc</th>\n",
       "      <th>...</th>\n",
       "      <th>contrast_del_mean</th>\n",
       "      <th>contrast_avg_sd</th>\n",
       "      <th>Tone1</th>\n",
       "      <th>Tone2</th>\n",
       "      <th>Tone3</th>\n",
       "      <th>Tone4</th>\n",
       "      <th>Tone5</th>\n",
       "      <th>Tone6</th>\n",
       "      <th>Tone_deltaMean</th>\n",
       "      <th>Tone_avg_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0800 HEAVEN</td>\n",
       "      <td>143.554688</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.6323</td>\n",
       "      <td>0.4358</td>\n",
       "      <td>2196.7699</td>\n",
       "      <td>2259.6592</td>\n",
       "      <td>862.0266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6804</td>\n",
       "      <td>6.128429</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>0.106067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 2 3 feat Jason Derulo  De La Ghetto</td>\n",
       "      <td>95.703125</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>2811.9937</td>\n",
       "      <td>2768.6767</td>\n",
       "      <td>1057.0624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>6.554386</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.008302</td>\n",
       "      <td>0.086733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              track_name       tempo  mean_zcr  median_zcr  \\\n",
       "0                            0800 HEAVEN  143.554688    0.0918      0.0913   \n",
       "1  1 2 3 feat Jason Derulo  De La Ghetto   95.703125    0.1278      0.1172   \n",
       "\n",
       "   std_zcr  max_zcr  aboveThr_zcr    mean_sc  median_sc     std_sc  ...  \\\n",
       "0   0.0527   0.6323        0.4358  2196.7699  2259.6592   862.0266  ...   \n",
       "1   0.0813   0.6621        0.6041  2811.9937  2768.6767  1057.0624  ...   \n",
       "\n",
       "   contrast_del_mean  contrast_avg_sd   Tone1   Tone2   Tone3   Tone4   Tone5  \\\n",
       "0             0.6804         6.128429  0.1183  0.1248  0.1614  0.1270  0.0527   \n",
       "1             0.7764         6.554386  0.1012  0.0931  0.1172  0.1136  0.0526   \n",
       "\n",
       "    Tone6  Tone_deltaMean  Tone_avg_sd  \n",
       "0  0.0522        0.009060     0.106067  \n",
       "1  0.0427        0.008302     0.086733  \n",
       "\n",
       "[2 rows x 71 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 'popularity_score_scaled' and 'time_frame' from df1 into df2\n",
    "df_model = df2.merge(df1[['track_name', 'popularity_score_scaled', 'time_frame']], \n",
    "                       on='track_name', \n",
    "                       how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>tempo</th>\n",
       "      <th>mean_zcr</th>\n",
       "      <th>median_zcr</th>\n",
       "      <th>std_zcr</th>\n",
       "      <th>max_zcr</th>\n",
       "      <th>aboveThr_zcr</th>\n",
       "      <th>mean_sc</th>\n",
       "      <th>median_sc</th>\n",
       "      <th>std_sc</th>\n",
       "      <th>...</th>\n",
       "      <th>Tone1</th>\n",
       "      <th>Tone2</th>\n",
       "      <th>Tone3</th>\n",
       "      <th>Tone4</th>\n",
       "      <th>Tone5</th>\n",
       "      <th>Tone6</th>\n",
       "      <th>Tone_deltaMean</th>\n",
       "      <th>Tone_avg_sd</th>\n",
       "      <th>popularity_score_scaled</th>\n",
       "      <th>time_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0800 HEAVEN</td>\n",
       "      <td>143.554688</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.6323</td>\n",
       "      <td>0.4358</td>\n",
       "      <td>2196.7699</td>\n",
       "      <td>2259.6592</td>\n",
       "      <td>862.0266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>0.106067</td>\n",
       "      <td>53.814449</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 2 3 feat Jason Derulo  De La Ghetto</td>\n",
       "      <td>95.703125</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>2811.9937</td>\n",
       "      <td>2768.6767</td>\n",
       "      <td>1057.0624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.008302</td>\n",
       "      <td>0.086733</td>\n",
       "      <td>54.638400</td>\n",
       "      <td>2117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              track_name       tempo  mean_zcr  median_zcr  \\\n",
       "0                            0800 HEAVEN  143.554688    0.0918      0.0913   \n",
       "1  1 2 3 feat Jason Derulo  De La Ghetto   95.703125    0.1278      0.1172   \n",
       "\n",
       "   std_zcr  max_zcr  aboveThr_zcr    mean_sc  median_sc     std_sc  ...  \\\n",
       "0   0.0527   0.6323        0.4358  2196.7699  2259.6592   862.0266  ...   \n",
       "1   0.0813   0.6621        0.6041  2811.9937  2768.6767  1057.0624  ...   \n",
       "\n",
       "    Tone1   Tone2   Tone3   Tone4   Tone5   Tone6  Tone_deltaMean  \\\n",
       "0  0.1183  0.1248  0.1614  0.1270  0.0527  0.0522        0.009060   \n",
       "1  0.1012  0.0931  0.1172  0.1136  0.0526  0.0427        0.008302   \n",
       "\n",
       "   Tone_avg_sd  popularity_score_scaled  time_frame  \n",
       "0     0.106067                53.814449       178.0  \n",
       "1     0.086733                54.638400      2117.0  \n",
       "\n",
       "[2 rows x 73 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# Plotting the distribution of popularity scores\n",
    "df_model['popularity_score_scaled'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the bin edges for the categories based on the provided ranges\n",
    "bins = [-np.inf, 30, 50, 60, np.inf]\n",
    "labels = ['Not_so_popular', 'Popular_During', 'Semi_Popular', 'Popular_Forever']\n",
    "\n",
    "# Create a new categorical column\n",
    "df_model['popularity_category'] = pd.cut(df_model['popularity_score_scaled'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_name</th>\n",
       "      <th>tempo</th>\n",
       "      <th>mean_zcr</th>\n",
       "      <th>median_zcr</th>\n",
       "      <th>std_zcr</th>\n",
       "      <th>max_zcr</th>\n",
       "      <th>aboveThr_zcr</th>\n",
       "      <th>mean_sc</th>\n",
       "      <th>median_sc</th>\n",
       "      <th>std_sc</th>\n",
       "      <th>...</th>\n",
       "      <th>Tone2</th>\n",
       "      <th>Tone3</th>\n",
       "      <th>Tone4</th>\n",
       "      <th>Tone5</th>\n",
       "      <th>Tone6</th>\n",
       "      <th>Tone_deltaMean</th>\n",
       "      <th>Tone_avg_sd</th>\n",
       "      <th>popularity_score_scaled</th>\n",
       "      <th>time_frame</th>\n",
       "      <th>popularity_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0800 HEAVEN</td>\n",
       "      <td>143.554688</td>\n",
       "      <td>0.0918</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.6323</td>\n",
       "      <td>0.4358</td>\n",
       "      <td>2196.7699</td>\n",
       "      <td>2259.6592</td>\n",
       "      <td>862.0266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>0.106067</td>\n",
       "      <td>53.814449</td>\n",
       "      <td>178.0</td>\n",
       "      <td>Semi_Popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 2 3 feat Jason Derulo  De La Ghetto</td>\n",
       "      <td>95.703125</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>2811.9937</td>\n",
       "      <td>2768.6767</td>\n",
       "      <td>1057.0624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.008302</td>\n",
       "      <td>0.086733</td>\n",
       "      <td>54.638400</td>\n",
       "      <td>2117.0</td>\n",
       "      <td>Semi_Popular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              track_name       tempo  mean_zcr  median_zcr  \\\n",
       "0                            0800 HEAVEN  143.554688    0.0918      0.0913   \n",
       "1  1 2 3 feat Jason Derulo  De La Ghetto   95.703125    0.1278      0.1172   \n",
       "\n",
       "   std_zcr  max_zcr  aboveThr_zcr    mean_sc  median_sc     std_sc  ...  \\\n",
       "0   0.0527   0.6323        0.4358  2196.7699  2259.6592   862.0266  ...   \n",
       "1   0.0813   0.6621        0.6041  2811.9937  2768.6767  1057.0624  ...   \n",
       "\n",
       "    Tone2   Tone3   Tone4   Tone5   Tone6  Tone_deltaMean  Tone_avg_sd  \\\n",
       "0  0.1248  0.1614  0.1270  0.0527  0.0522        0.009060     0.106067   \n",
       "1  0.0931  0.1172  0.1136  0.0526  0.0427        0.008302     0.086733   \n",
       "\n",
       "   popularity_score_scaled  time_frame  popularity_category  \n",
       "0                53.814449       178.0         Semi_Popular  \n",
       "1                54.638400      2117.0         Semi_Popular  \n",
       "\n",
       "[2 rows x 74 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARING THE DATA\n",
    "\n",
    "# Features and target\n",
    "X = df_model.drop(['track_name', 'popularity_score_scaled', 'popularity_category'], axis=1)\n",
    "y = df_model['popularity_category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "df_scaled['popularity_category'] = y\n",
    "df_scaled['track_name'] = df_model['track_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLITTING THE DATASET\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42718446601941745\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Not_so_popular       0.47      0.32      0.38        25\n",
      " Popular_During       0.47      0.67      0.55        51\n",
      "Popular_Forever       0.00      0.00      0.00         8\n",
      "   Semi_Popular       0.20      0.11      0.14        19\n",
      "\n",
      "       accuracy                           0.43       103\n",
      "      macro avg       0.29      0.27      0.27       103\n",
      "   weighted avg       0.38      0.43      0.39       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################### LOGISTIC REGRESSION ######################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the Logistic Regression model achieved an overall accuracy of 42.7% on test data, which suggests it performs better than random guessing but is still not highly accurate. The precision, recall, and f1-score for each class provide additional insight into the model's performance:\n",
    "\n",
    "Not_so_popular: Has a precision of 0.47 and recall of 0.32, indicating that the model is relatively conservative when predicting this class; it's correct less than half the time when it does predict this class, and it identifies 32% of all actual instances of this class.\n",
    "\n",
    "Popular_During: This category has the best f1-score at 0.55 with a precision of 0.47 and a recall of 0.67, suggesting that the model is more confident and accurate in predicting this class than the others.\n",
    "\n",
    "Popular_Forever: The model did not correctly predict any instances of this class (precision and recall are both 0). This could be due to the reason that the class is underrepresented in the dataset.\n",
    "\n",
    "Semi_Popular: It seems there is a fourth category in the output that wasn't mentioned before. For this category, the model has a precision of 0.20 and recall of 0.11, indicating poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_model['popularity_category'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance is a common issue in machine learning classification problems and can lead to poor model performance, especially for the minority class. \n",
    "Techniques like SMOTE for oversampling, class weights during training, or collecting more data can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_scaled is your feature set and y is the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Fit SMOTE to the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34951456310679613\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Not_so_popular       0.43      0.40      0.42        25\n",
      " Popular_During       0.50      0.39      0.44        51\n",
      "Popular_Forever       0.15      0.25      0.19         8\n",
      "   Semi_Popular       0.15      0.21      0.17        19\n",
      "\n",
      "       accuracy                           0.35       103\n",
      "      macro avg       0.31      0.31      0.31       103\n",
      "   weighted avg       0.39      0.35      0.37       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################### LOGISTIC REGRESSION ######################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new results, the precision and recall for the \"Popular_Forever\" and \"Semi_Popular\" categories have increased, which means the model is now better at identifying these previously under-represented classes. This is an indication that the balancing technique is having the desired effect on the model's ability to classify the minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.5145631067961165\n",
      "Random Forest Classifier Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Not_so_popular       0.58      0.44      0.50        25\n",
      " Popular_During       0.56      0.71      0.63        51\n",
      "Popular_Forever       0.00      0.00      0.00         8\n",
      "   Semi_Popular       0.38      0.32      0.34        19\n",
      "\n",
      "       accuracy                           0.51       103\n",
      "      macro avg       0.38      0.37      0.37       103\n",
      "   weighted avg       0.49      0.51      0.49       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################################################## RANDOM FOREST CLASSIFIER ##############################################################################################################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_rf)\n",
    "print(\"Random Forest Classifier Classification Report:\")\n",
    "print(report_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the hyper parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Score: 0.81067511129469\n",
      "Best Random Forest Classifier Accuracy: 0.4174757281553398\n",
      "Best Random Forest Classifier Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Not_so_popular       0.47      0.32      0.38        25\n",
      " Popular_During       0.50      0.63      0.56        51\n",
      "Popular_Forever       0.00      0.00      0.00         8\n",
      "   Semi_Popular       0.19      0.16      0.17        19\n",
      "\n",
      "       accuracy                           0.42       103\n",
      "      macro avg       0.29      0.28      0.28       103\n",
      "   weighted avg       0.40      0.42      0.40       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Use the best estimator for making predictions\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "report_best_rf = classification_report(y_test, y_pred_best_rf)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Best Random Forest Classifier Accuracy:\", accuracy_best_rf)\n",
    "print(\"Best Random Forest Classifier Classification Report:\")\n",
    "print(report_best_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy: 0.4854368932038835\n",
      "KNN Classifier Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      " Not_so_popular       0.32      0.24      0.27        25\n",
      " Popular_During       0.53      0.76      0.63        51\n",
      "Popular_Forever       0.00      0.00      0.00         8\n",
      "   Semi_Popular       0.50      0.26      0.34        19\n",
      "\n",
      "       accuracy                           0.49       103\n",
      "      macro avg       0.34      0.32      0.31       103\n",
      "   weighted avg       0.43      0.49      0.44       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNeighborsClassifier\n",
    "# n_neighbors is a hyperparameter that you can tune. Starting with 5.\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "report_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"KNN Classifier Accuracy:\", accuracy_knn)\n",
    "print(\"KNN Classifier Classification Report:\")\n",
    "print(report_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training target labels\n",
    "y_train_smote_encoded = le.fit_transform(y_train_smote)\n",
    "\n",
    "# Transform the test target labels (do not fit on test data)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Now use y_train_smote_encoded for training and y_test_encoded for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "73/73 [==============================] - 1s 4ms/step - loss: 1.5024 - accuracy: 0.2655 - val_loss: 1.7752 - val_accuracy: 0.1358\n",
      "Epoch 2/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.3358 - accuracy: 0.3769 - val_loss: 1.5746 - val_accuracy: 0.1481\n",
      "Epoch 3/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.2483 - accuracy: 0.4237 - val_loss: 1.5556 - val_accuracy: 0.1728\n",
      "Epoch 4/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.1830 - accuracy: 0.4911 - val_loss: 1.4975 - val_accuracy: 0.1605\n",
      "Epoch 5/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.1293 - accuracy: 0.5158 - val_loss: 1.4649 - val_accuracy: 0.2099\n",
      "Epoch 6/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0817 - accuracy: 0.5406 - val_loss: 1.4368 - val_accuracy: 0.2593\n",
      "Epoch 7/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.0402 - accuracy: 0.5653 - val_loss: 1.3542 - val_accuracy: 0.3457\n",
      "Epoch 8/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.9964 - accuracy: 0.6121 - val_loss: 1.3474 - val_accuracy: 0.3704\n",
      "Epoch 9/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.9568 - accuracy: 0.6272 - val_loss: 1.2516 - val_accuracy: 0.4198\n",
      "Epoch 10/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.9172 - accuracy: 0.6547 - val_loss: 1.2485 - val_accuracy: 0.4444\n",
      "Epoch 11/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8826 - accuracy: 0.6685 - val_loss: 1.1993 - val_accuracy: 0.4938\n",
      "Epoch 12/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8498 - accuracy: 0.6602 - val_loss: 1.1352 - val_accuracy: 0.5432\n",
      "Epoch 13/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8205 - accuracy: 0.6878 - val_loss: 1.0838 - val_accuracy: 0.5432\n",
      "Epoch 14/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7946 - accuracy: 0.7029 - val_loss: 1.0598 - val_accuracy: 0.5679\n",
      "Epoch 15/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7685 - accuracy: 0.7098 - val_loss: 0.9900 - val_accuracy: 0.5926\n",
      "Epoch 16/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7418 - accuracy: 0.7139 - val_loss: 0.9687 - val_accuracy: 0.6049\n",
      "Epoch 17/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7235 - accuracy: 0.7235 - val_loss: 0.9581 - val_accuracy: 0.6296\n",
      "Epoch 18/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7041 - accuracy: 0.7345 - val_loss: 0.9260 - val_accuracy: 0.6173\n",
      "Epoch 19/150\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6788 - accuracy: 0.7469 - val_loss: 0.9134 - val_accuracy: 0.6543\n",
      "Epoch 20/150\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.6592 - accuracy: 0.7510 - val_loss: 0.8694 - val_accuracy: 0.6790\n",
      "Epoch 21/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.6399 - accuracy: 0.7689 - val_loss: 0.8761 - val_accuracy: 0.6420\n",
      "Epoch 22/150\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.6201 - accuracy: 0.7730 - val_loss: 0.8123 - val_accuracy: 0.7037\n",
      "Epoch 23/150\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.6052 - accuracy: 0.7717 - val_loss: 0.7765 - val_accuracy: 0.7778\n",
      "Epoch 24/150\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.5932 - accuracy: 0.7813 - val_loss: 0.7818 - val_accuracy: 0.7407\n",
      "Epoch 25/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.5756 - accuracy: 0.7854 - val_loss: 0.7288 - val_accuracy: 0.7778\n",
      "Epoch 26/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.5557 - accuracy: 0.7937 - val_loss: 0.7211 - val_accuracy: 0.7901\n",
      "Epoch 27/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.5464 - accuracy: 0.8061 - val_loss: 0.7052 - val_accuracy: 0.8025\n",
      "Epoch 28/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.5339 - accuracy: 0.8061 - val_loss: 0.6720 - val_accuracy: 0.8395\n",
      "Epoch 29/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5189 - accuracy: 0.8143 - val_loss: 0.6565 - val_accuracy: 0.8519\n",
      "Epoch 30/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.5073 - accuracy: 0.8267 - val_loss: 0.6318 - val_accuracy: 0.8519\n",
      "Epoch 31/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.4982 - accuracy: 0.8184 - val_loss: 0.6773 - val_accuracy: 0.8025\n",
      "Epoch 32/150\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4918 - accuracy: 0.8308 - val_loss: 0.6388 - val_accuracy: 0.8272\n",
      "Epoch 33/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4756 - accuracy: 0.8322 - val_loss: 0.5807 - val_accuracy: 0.8642\n",
      "Epoch 34/150\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4654 - accuracy: 0.8432 - val_loss: 0.5612 - val_accuracy: 0.8765\n",
      "Epoch 35/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4560 - accuracy: 0.8446 - val_loss: 0.5824 - val_accuracy: 0.8642\n",
      "Epoch 36/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4510 - accuracy: 0.8418 - val_loss: 0.5517 - val_accuracy: 0.8642\n",
      "Epoch 37/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.4381 - accuracy: 0.8611 - val_loss: 0.5225 - val_accuracy: 0.9012\n",
      "Epoch 38/150\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4308 - accuracy: 0.8556 - val_loss: 0.5134 - val_accuracy: 0.9012\n",
      "Epoch 39/150\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4254 - accuracy: 0.8514 - val_loss: 0.4996 - val_accuracy: 0.8889\n",
      "Epoch 40/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4147 - accuracy: 0.8528 - val_loss: 0.4860 - val_accuracy: 0.9136\n",
      "Epoch 41/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4084 - accuracy: 0.8556 - val_loss: 0.4879 - val_accuracy: 0.9012\n",
      "Epoch 42/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3976 - accuracy: 0.8666 - val_loss: 0.4381 - val_accuracy: 0.9259\n",
      "Epoch 43/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3961 - accuracy: 0.8762 - val_loss: 0.4658 - val_accuracy: 0.9012\n",
      "Epoch 44/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3877 - accuracy: 0.8721 - val_loss: 0.4496 - val_accuracy: 0.9259\n",
      "Epoch 45/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3768 - accuracy: 0.8762 - val_loss: 0.4636 - val_accuracy: 0.9259\n",
      "Epoch 46/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3750 - accuracy: 0.8762 - val_loss: 0.4320 - val_accuracy: 0.9383\n",
      "Epoch 47/150\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.3670 - accuracy: 0.8790 - val_loss: 0.3987 - val_accuracy: 0.9383\n",
      "Epoch 48/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3592 - accuracy: 0.8913 - val_loss: 0.4393 - val_accuracy: 0.9259\n",
      "Epoch 49/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3547 - accuracy: 0.8900 - val_loss: 0.4139 - val_accuracy: 0.9259\n",
      "Epoch 50/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.3476 - accuracy: 0.8858 - val_loss: 0.4286 - val_accuracy: 0.9383\n",
      "Epoch 51/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.3447 - accuracy: 0.8955 - val_loss: 0.4077 - val_accuracy: 0.9259\n",
      "Epoch 52/150\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.3350 - accuracy: 0.8941 - val_loss: 0.3729 - val_accuracy: 0.9383\n",
      "Epoch 53/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.3289 - accuracy: 0.8968 - val_loss: 0.3802 - val_accuracy: 0.9383\n",
      "Epoch 54/150\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.3185 - accuracy: 0.9106 - val_loss: 0.3647 - val_accuracy: 0.9506\n",
      "Epoch 55/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3177 - accuracy: 0.9023 - val_loss: 0.3704 - val_accuracy: 0.9506\n",
      "Epoch 56/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.3091 - accuracy: 0.9092 - val_loss: 0.3290 - val_accuracy: 0.9630\n",
      "Epoch 57/150\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.3020 - accuracy: 0.9065 - val_loss: 0.3624 - val_accuracy: 0.9506\n",
      "Epoch 58/150\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.2999 - accuracy: 0.9133 - val_loss: 0.3391 - val_accuracy: 0.9630\n",
      "Epoch 59/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.2911 - accuracy: 0.9133 - val_loss: 0.3323 - val_accuracy: 0.9630\n",
      "Epoch 60/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.2877 - accuracy: 0.9133 - val_loss: 0.3290 - val_accuracy: 0.9630\n",
      "Epoch 61/150\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.2785 - accuracy: 0.9230 - val_loss: 0.3278 - val_accuracy: 0.9630\n",
      "Epoch 62/150\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.2759 - accuracy: 0.9202 - val_loss: 0.3353 - val_accuracy: 0.9506\n",
      "Epoch 63/150\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.2712 - accuracy: 0.9216 - val_loss: 0.3180 - val_accuracy: 0.9630\n",
      "Epoch 64/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.2642 - accuracy: 0.9230 - val_loss: 0.2991 - val_accuracy: 0.9630\n",
      "Epoch 65/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.2601 - accuracy: 0.9257 - val_loss: 0.3056 - val_accuracy: 0.9630\n",
      "Epoch 66/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.2534 - accuracy: 0.9298 - val_loss: 0.3044 - val_accuracy: 0.9753\n",
      "Epoch 67/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.2514 - accuracy: 0.9257 - val_loss: 0.2922 - val_accuracy: 0.9753\n",
      "Epoch 68/150\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.2449 - accuracy: 0.9312 - val_loss: 0.2561 - val_accuracy: 0.9753\n",
      "Epoch 69/150\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.2408 - accuracy: 0.9312 - val_loss: 0.3071 - val_accuracy: 0.9383\n",
      "Epoch 70/150\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.2360 - accuracy: 0.9354 - val_loss: 0.3113 - val_accuracy: 0.9506\n",
      "Epoch 71/150\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9340 - val_loss: 0.2951 - val_accuracy: 0.9630\n",
      "Epoch 72/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.2232 - accuracy: 0.9395 - val_loss: 0.2689 - val_accuracy: 0.9753\n",
      "Epoch 73/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.2229 - accuracy: 0.9354 - val_loss: 0.2860 - val_accuracy: 0.9753\n",
      "Epoch 74/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.2185 - accuracy: 0.9354 - val_loss: 0.2387 - val_accuracy: 0.9877\n",
      "Epoch 75/150\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.2122 - accuracy: 0.9436 - val_loss: 0.2511 - val_accuracy: 0.9753\n",
      "Epoch 76/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.2130 - accuracy: 0.9367 - val_loss: 0.2809 - val_accuracy: 0.9506\n",
      "Epoch 77/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.2042 - accuracy: 0.9450 - val_loss: 0.2667 - val_accuracy: 0.9753\n",
      "Epoch 78/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.1998 - accuracy: 0.9477 - val_loss: 0.2842 - val_accuracy: 0.9506\n",
      "Epoch 79/150\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.1940 - accuracy: 0.9477 - val_loss: 0.2501 - val_accuracy: 0.9753\n",
      "Epoch 80/150\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.1892 - accuracy: 0.9477 - val_loss: 0.2604 - val_accuracy: 0.9630\n",
      "Epoch 81/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.1848 - accuracy: 0.9519 - val_loss: 0.2345 - val_accuracy: 0.9630\n",
      "Epoch 82/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1828 - accuracy: 0.9519 - val_loss: 0.2745 - val_accuracy: 0.9506\n",
      "Epoch 83/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1797 - accuracy: 0.9519 - val_loss: 0.2840 - val_accuracy: 0.9506\n",
      "Epoch 84/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1741 - accuracy: 0.9560 - val_loss: 0.2602 - val_accuracy: 0.9383\n",
      "Epoch 85/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1697 - accuracy: 0.9574 - val_loss: 0.2635 - val_accuracy: 0.9630\n",
      "Epoch 86/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.1703 - accuracy: 0.9615 - val_loss: 0.2416 - val_accuracy: 0.9753\n",
      "Epoch 87/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.1643 - accuracy: 0.9629 - val_loss: 0.2613 - val_accuracy: 0.9506\n",
      "Epoch 88/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1603 - accuracy: 0.9560 - val_loss: 0.2434 - val_accuracy: 0.9630\n",
      "Epoch 89/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.1563 - accuracy: 0.9642 - val_loss: 0.2524 - val_accuracy: 0.9753\n",
      "Epoch 90/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.1522 - accuracy: 0.9656 - val_loss: 0.2450 - val_accuracy: 0.9630\n",
      "Epoch 91/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1543 - accuracy: 0.9642 - val_loss: 0.2490 - val_accuracy: 0.9630\n",
      "Epoch 92/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1458 - accuracy: 0.9656 - val_loss: 0.2280 - val_accuracy: 0.9753\n",
      "Epoch 93/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.1409 - accuracy: 0.9642 - val_loss: 0.2481 - val_accuracy: 0.9630\n",
      "Epoch 94/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1387 - accuracy: 0.9684 - val_loss: 0.2136 - val_accuracy: 0.9630\n",
      "Epoch 95/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9697 - val_loss: 0.2182 - val_accuracy: 0.9630\n",
      "Epoch 96/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1349 - accuracy: 0.9670 - val_loss: 0.2440 - val_accuracy: 0.9630\n",
      "Epoch 97/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1286 - accuracy: 0.9711 - val_loss: 0.2199 - val_accuracy: 0.9753\n",
      "Epoch 98/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.1257 - accuracy: 0.9725 - val_loss: 0.2397 - val_accuracy: 0.9630\n",
      "Epoch 99/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.1231 - accuracy: 0.9739 - val_loss: 0.2363 - val_accuracy: 0.9630\n",
      "Epoch 100/150\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.1232 - accuracy: 0.9697 - val_loss: 0.2209 - val_accuracy: 0.9630\n",
      "Epoch 101/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1199 - accuracy: 0.9697 - val_loss: 0.2428 - val_accuracy: 0.9630\n",
      "Epoch 102/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1176 - accuracy: 0.9697 - val_loss: 0.2388 - val_accuracy: 0.9630\n",
      "Epoch 103/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1130 - accuracy: 0.9752 - val_loss: 0.2537 - val_accuracy: 0.9630\n",
      "Epoch 104/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.1101 - accuracy: 0.9752 - val_loss: 0.2417 - val_accuracy: 0.9630\n",
      "Epoch 105/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.1078 - accuracy: 0.9752 - val_loss: 0.2274 - val_accuracy: 0.9630\n",
      "Epoch 106/150\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.1068 - accuracy: 0.9752 - val_loss: 0.2152 - val_accuracy: 0.9630\n",
      "Epoch 107/150\n",
      "73/73 [==============================] - 0s 7ms/step - loss: 0.1028 - accuracy: 0.9752 - val_loss: 0.2108 - val_accuracy: 0.9630\n",
      "Epoch 108/150\n",
      "73/73 [==============================] - 0s 7ms/step - loss: 0.1001 - accuracy: 0.9752 - val_loss: 0.2192 - val_accuracy: 0.9630\n",
      "Epoch 109/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.1002 - accuracy: 0.9725 - val_loss: 0.2316 - val_accuracy: 0.9630\n",
      "Epoch 110/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9780 - val_loss: 0.2210 - val_accuracy: 0.9630\n",
      "Epoch 111/150\n",
      "73/73 [==============================] - 0s 6ms/step - loss: 0.0925 - accuracy: 0.9794 - val_loss: 0.2139 - val_accuracy: 0.9630\n",
      "Epoch 112/150\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0894 - accuracy: 0.9807 - val_loss: 0.2421 - val_accuracy: 0.9630\n",
      "Epoch 113/150\n",
      "73/73 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9766 - val_loss: 0.2198 - val_accuracy: 0.9630\n",
      "Epoch 114/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0868 - accuracy: 0.9807 - val_loss: 0.2062 - val_accuracy: 0.9506\n",
      "Epoch 115/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0875 - accuracy: 0.9752 - val_loss: 0.2237 - val_accuracy: 0.9506\n",
      "Epoch 116/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0818 - accuracy: 0.9794 - val_loss: 0.2343 - val_accuracy: 0.9506\n",
      "Epoch 117/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.0805 - accuracy: 0.9849 - val_loss: 0.2411 - val_accuracy: 0.9506\n",
      "Epoch 118/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0774 - accuracy: 0.9807 - val_loss: 0.2302 - val_accuracy: 0.9506\n",
      "Epoch 119/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0766 - accuracy: 0.9807 - val_loss: 0.1990 - val_accuracy: 0.9630\n",
      "Epoch 120/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0744 - accuracy: 0.9821 - val_loss: 0.2385 - val_accuracy: 0.9506\n",
      "Epoch 121/150\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.0713 - accuracy: 0.9821 - val_loss: 0.2396 - val_accuracy: 0.9506\n",
      "Epoch 122/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0685 - accuracy: 0.9835 - val_loss: 0.2244 - val_accuracy: 0.9506\n",
      "Epoch 123/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0676 - accuracy: 0.9862 - val_loss: 0.2186 - val_accuracy: 0.9506\n",
      "Epoch 124/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0676 - accuracy: 0.9876 - val_loss: 0.2154 - val_accuracy: 0.9506\n",
      "Epoch 125/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0647 - accuracy: 0.9835 - val_loss: 0.2284 - val_accuracy: 0.9506\n",
      "Epoch 126/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0657 - accuracy: 0.9890 - val_loss: 0.1934 - val_accuracy: 0.9506\n",
      "Epoch 127/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0621 - accuracy: 0.9835 - val_loss: 0.2317 - val_accuracy: 0.9506\n",
      "Epoch 128/150\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.0616 - accuracy: 0.9849 - val_loss: 0.2066 - val_accuracy: 0.9506\n",
      "Epoch 129/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0592 - accuracy: 0.9862 - val_loss: 0.2149 - val_accuracy: 0.9506\n",
      "Epoch 130/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.0575 - accuracy: 0.9876 - val_loss: 0.2216 - val_accuracy: 0.9506\n",
      "Epoch 131/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0567 - accuracy: 0.9876 - val_loss: 0.2359 - val_accuracy: 0.9506\n",
      "Epoch 132/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0558 - accuracy: 0.9862 - val_loss: 0.2181 - val_accuracy: 0.9506\n",
      "Epoch 133/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0541 - accuracy: 0.9904 - val_loss: 0.2206 - val_accuracy: 0.9506\n",
      "Epoch 134/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0526 - accuracy: 0.9904 - val_loss: 0.2126 - val_accuracy: 0.9506\n",
      "Epoch 135/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0517 - accuracy: 0.9904 - val_loss: 0.2196 - val_accuracy: 0.9506\n",
      "Epoch 136/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0496 - accuracy: 0.9890 - val_loss: 0.1990 - val_accuracy: 0.9506\n",
      "Epoch 137/150\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.0485 - accuracy: 0.9904 - val_loss: 0.2003 - val_accuracy: 0.9506\n",
      "Epoch 138/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0480 - accuracy: 0.9904 - val_loss: 0.2188 - val_accuracy: 0.9506\n",
      "Epoch 139/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.0481 - accuracy: 0.9904 - val_loss: 0.2207 - val_accuracy: 0.9506\n",
      "Epoch 140/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0455 - accuracy: 0.9904 - val_loss: 0.2164 - val_accuracy: 0.9506\n",
      "Epoch 141/150\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.0462 - accuracy: 0.9904 - val_loss: 0.1971 - val_accuracy: 0.9506\n",
      "Epoch 142/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0532 - accuracy: 0.9876 - val_loss: 0.2104 - val_accuracy: 0.9383\n",
      "Epoch 143/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0609 - accuracy: 0.9821 - val_loss: 0.2645 - val_accuracy: 0.9259\n",
      "Epoch 144/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0551 - accuracy: 0.9876 - val_loss: 0.2524 - val_accuracy: 0.9259\n",
      "Epoch 145/150\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.0456 - accuracy: 0.9890 - val_loss: 0.2575 - val_accuracy: 0.9383\n",
      "Epoch 146/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0418 - accuracy: 0.9904 - val_loss: 0.2368 - val_accuracy: 0.9506\n",
      "Epoch 147/150\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.0403 - accuracy: 0.9904 - val_loss: 0.2114 - val_accuracy: 0.9506\n",
      "Epoch 148/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0390 - accuracy: 0.9917 - val_loss: 0.2125 - val_accuracy: 0.9506\n",
      "Epoch 149/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0376 - accuracy: 0.9917 - val_loss: 0.2238 - val_accuracy: 0.9259\n",
      "Epoch 150/150\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.0365 - accuracy: 0.9917 - val_loss: 0.2130 - val_accuracy: 0.9383\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 7.4345 - accuracy: 0.3786\n",
      "Neural Network Accuracy: 0.3786407709121704\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X_train_smote.shape[1], activation='relu'))  # First hidden layer with 12 neurons\n",
    "model.add(Dense(8, activation='relu'))  # Second hidden layer with 8 neurons\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer with 4 neurons since we have 4 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training data\n",
    "history = model.fit(X_train_smote, y_train_smote_encoded, validation_split=0.1, epochs=150, batch_size=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Neural Network Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_smote.shape[1], activation='relu', kernel_regularizer=l2(0.001)))  # Increased neurons and added L2 regularization\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))  # Another hidden layer\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))  # Output layer remains the same\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training data\n",
    "history = model.fit(X_train_smote, y_train_smote_encoded, validation_split=0.1, epochs=100, batch_size=32, verbose=1)  # Adjusted epochs and batch size\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def create_model(learning_rate=0.001, activation='relu', dropout_rate=0.0, neurons=32):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X_train_smote.shape[1], activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, activation='softmax'))  # Assuming 4 output classes\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'dropout_rate': [0.0, 0.3, 0.5],\n",
    "    'neurons': [32, 64, 128],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_jobs=-1, cv=3, n_iter=10)\n",
    "random_search_results = random_search.fit(X_train_smote, y_train_smote_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.745248 using {'neurons': 64, 'learning_rate': 0.01, 'epochs': 150, 'dropout_rate': 0.0, 'batch_size': 32, 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (random_search_results.best_score_, random_search_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 1.3224 - accuracy: 0.4332\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.9085 - accuracy: 0.6386\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.6612 - accuracy: 0.7438\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.5505 - accuracy: 0.7871\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8762\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9233\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9443\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9777\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9851\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 0s 560us/step - loss: 0.0720 - accuracy: 0.9814\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 0s 665us/step - loss: 0.0681 - accuracy: 0.9876\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 0s 821us/step - loss: 0.0337 - accuracy: 0.9926\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 0s 796us/step - loss: 0.0292 - accuracy: 0.9950\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9988\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 0s 631us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 0s 706us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 0s 658us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 0s 804us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 0s 889us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 9.6752e-04 - accuracy: 1.0000\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 9.1004e-04 - accuracy: 1.0000\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 8.5612e-04 - accuracy: 1.0000\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 8.0267e-04 - accuracy: 1.0000\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 7.5788e-04 - accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 0s 819us/step - loss: 7.1746e-04 - accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 0s 843us/step - loss: 6.7577e-04 - accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 0s 863us/step - loss: 6.4096e-04 - accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 6.0810e-04 - accuracy: 1.0000\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 5.8036e-04 - accuracy: 1.0000\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 5.5061e-04 - accuracy: 1.0000\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 5.2429e-04 - accuracy: 1.0000\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 5.0227e-04 - accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 4.7779e-04 - accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 0s 734us/step - loss: 4.5632e-04 - accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 4.3656e-04 - accuracy: 1.0000\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 4.1818e-04 - accuracy: 1.0000\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 4.0068e-04 - accuracy: 1.0000\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.8368e-04 - accuracy: 1.0000\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.6927e-04 - accuracy: 1.0000\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.5329e-04 - accuracy: 1.0000\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.4002e-04 - accuracy: 1.0000\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.2836e-04 - accuracy: 1.0000\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 3.1445e-04 - accuracy: 1.0000\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 3.0204e-04 - accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 2.9112e-04 - accuracy: 1.0000\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2.8027e-04 - accuracy: 1.0000\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2.7029e-04 - accuracy: 1.0000\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2.6079e-04 - accuracy: 1.0000\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 2.5194e-04 - accuracy: 1.0000\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 2.4344e-04 - accuracy: 1.0000\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2.3525e-04 - accuracy: 1.0000\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 2.2740e-04 - accuracy: 1.0000\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 2.1982e-04 - accuracy: 1.0000\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.1239e-04 - accuracy: 1.0000\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 2.0541e-04 - accuracy: 1.0000\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.9935e-04 - accuracy: 1.0000\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.9270e-04 - accuracy: 1.0000\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.8699e-04 - accuracy: 1.0000\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.8135e-04 - accuracy: 1.0000\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.7622e-04 - accuracy: 1.0000\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.7042e-04 - accuracy: 1.0000\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.6505e-04 - accuracy: 1.0000\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.6061e-04 - accuracy: 1.0000\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.5577e-04 - accuracy: 1.0000\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.5120e-04 - accuracy: 1.0000\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.4724e-04 - accuracy: 1.0000\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.4263e-04 - accuracy: 1.0000\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.3865e-04 - accuracy: 1.0000\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 1.3492e-04 - accuracy: 1.0000\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3110e-04 - accuracy: 1.0000\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2734e-04 - accuracy: 1.0000\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2394e-04 - accuracy: 1.0000\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2067e-04 - accuracy: 1.0000\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1751e-04 - accuracy: 1.0000\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.1448e-04 - accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1165e-04 - accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0861e-04 - accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0565e-04 - accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 1.0314e-04 - accuracy: 1.0000\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0039e-04 - accuracy: 1.0000\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 9.7967e-05 - accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 9.5374e-05 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 9.3065e-05 - accuracy: 1.0000\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 9.0867e-05 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 8.8544e-05 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 8.6300e-05 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 8.4310e-05 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 8.2209e-05 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 8.0374e-05 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 7.8343e-05 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 7.6585e-05 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 7.4713e-05 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 7.3114e-05 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 7.1382e-05 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 6.9743e-05 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 6.8220e-05 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 6.6574e-05 - accuracy: 1.0000\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 6.5239e-05 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 6.3737e-05 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 6.2348e-05 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 6.0912e-05 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 5.9556e-05 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 5.8353e-05 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 5.7002e-05 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 5.5826e-05 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 5.4546e-05 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 5.3375e-05 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 5.2275e-05 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 5.1120e-05 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 5.0009e-05 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.9028e-05 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.8086e-05 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 4.6968e-05 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.6012e-05 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.5159e-05 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.4093e-05 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.3202e-05 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 4.2358e-05 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.1466e-05 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 4.0630e-05 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.9802e-05 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 3.8931e-05 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.8225e-05 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.7420e-05 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.6707e-05 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 3.6018e-05 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 3.5240e-05 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.4623e-05 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.3851e-05 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 3.3274e-05 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.2585e-05 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.1982e-05 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 3.1304e-05 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 3.0790e-05 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 3.0171e-05 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 2.9569e-05 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 2.8992e-05 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 2.8439e-05 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 2.7876e-05 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 2.7364e-05 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 2.6881e-05 - accuracy: 1.0000\n",
      "4/4 [==============================] - 1s 9ms/step - loss: 4.8403 - accuracy: 0.3883\n",
      "Final model accuracy: 0.3883495032787323\n"
     ]
    }
   ],
   "source": [
    "# Recreate the best model using the optimal hyperparameters\n",
    "best_model = create_model(\n",
    "    learning_rate=0.01,\n",
    "    activation='tanh',\n",
    "    dropout_rate=0.0,\n",
    "    neurons=64\n",
    ")\n",
    "\n",
    "# Train the model with the full training dataset\n",
    "history = best_model.fit(X_train_smote, y_train_smote_encoded, epochs=150, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test_encoded)\n",
    "print(f'Final model accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.40      0.39        25\n",
      "           1       0.50      0.49      0.50        51\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.26      0.26      0.26        19\n",
      "\n",
      "    accuracy                           0.39       103\n",
      "   macro avg       0.29      0.29      0.29       103\n",
      "weighted avg       0.39      0.39      0.39       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test_encoded, y_pred)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection<br/>\n",
    "Try different algorithms to see which performs best. Common choices for regression problems include:<br/>\n",
    "\n",
    "Linear Regression<br/>\n",
    "Decision Tree Regression<br/>\n",
    "Random Forest Regression<br/>\n",
    "Gradient Boosting Machines (like XGBoost or LightGBM)<br/>\n",
    "Support Vector Machines (SVM)<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression trained.\n",
      "Random Forest trained.\n",
      "Support Vector Machine trained.\n",
      "XGBoost trained.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"XGBoost\": XGBRegressor()\n",
    "}\n",
    "\n",
    "# Train models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name} trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - MSE: 232.2233027153926, R2: 0.046319133218791886\n",
      "Random Forest - MSE: 179.32754567529133, R2: 0.2635482865087052\n",
      "Support Vector Machine - MSE: 239.72273652360624, R2: 0.015520903881097503\n",
      "XGBoost - MSE: 208.5506806497821, R2: 0.1435364514919032\n"
     ]
    }
   ],
   "source": [
    "### MODEL EVALUATION\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} - MSE: {mse}, R2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor:\n",
    "- MSE: 188.11\n",
    "- R2: 0.227\n",
    "\n",
    "This model has the lowest MSE and the highest R² score, indicating it's the best at predicting the 'popularity_score_scaled' among the models you tested.\n",
    "Linear Regression and Support Vector Machine:\n",
    "\n",
    "These models have higher MSEs and lower R² scores, suggesting they're not capturing the complexity of your data as effectively as the Random Forest.\n",
    "XGBoost:<br/>\n",
    "\n",
    "XGBoost shows a decent performance but is still outperformed by the Random Forest. It might improve with hyperparameter tuning.<br/>\n",
    "Next Steps:<br/>\n",
    "1. Hyperparameter Tuning:<br/>\n",
    "The performance of the Random Forest and XGBoost models might be significantly improved by tuning their hyperparameters. You can use techniques like Grid Search or Random Search for this purpose.\n",
    "2. Feature Importance Analysis:<br/>\n",
    "Especially for the Random Forest and XGBoost models, check which features are most important for predicting popularity. This could provide insights into your dataset and might even suggest further feature engineering or selection.\n",
    "3. Cross-Validation:<br/>\n",
    "Consider using cross-validation to assess the model's performance. This will give you a more robust understanding of how well the model might perform on unseen data.\n",
    "4. Model Refinement:<br/>\n",
    "Based on the results of hyperparameter tuning and feature importance, refine your models. You may also want to revisit data preprocessing steps if you think there's more room for improvement.\n",
    "5. Final Model Selection:<br/>\n",
    "Once you've tuned the models and reassessed their performance, choose the one that shows the best results on your test data for your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "###### RUNNING ON COLLAB\n",
    "\"\"\"\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [10, 20, 30, 40, 50, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", rf_random.best_params_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 0.9, 1],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = param_grid, \n",
    "                                n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "xgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", xgb_random.best_params_)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
